# -*- coding: utf-8 -*-
"""Copy of Data Processing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16UpaezzZLrgtc-fWATUDcz6MrMw3eghn
"""

from google.colab import drive
drive.mount('/content/drive')

"""Import Library"""

!pip install geopandas
!pip install mapclassify

import pandas as pd
import geopandas as gpd
from shapely.geometry import Point, Polygon
import numpy as np
import matplotlib.pyplot as plt
from shapely import wkt

"""# Data Cleaning

## Criminal Records Dataset
"""

# read crime dataset
data = pd.read_excel('/content/drive/MyDrive/FIT5120 Team TA25 \
                      Project Governance Portfolio/System Architecture \
                      and Security/Data Governance/Open Dataset/\
                      Data_Tables_LGA_Criminal_Incidents_Year_Ending_March_2022.xlsx', \
                      sheet_name='Table 03')

# review the dataset
print("Dataset dimension: ", data.shape)
data.head()

"""### Check data"""

# check for the missing data
data.isnull().any()

# check for duplicated data in the dataset
data.duplicated().any()

"""### Make Modification to the Data"""

# get the value of the offence division in the data
data['Offence Division'].unique()

# Remove first 2 character (sub title)
data['Offence Division'] = data['Offence Division'].str[2:]
data['Offence Division'].unique()

# filter the data to crime against the person only
against_person_data = data[data['Offence Division'] == 'Crimes against the person']
against_person_data.head()

# check the value of the crime subdivision
against_person_data['Offence Subdivision'].unique()

# remove the sub-heading of the subdivision criteria
against_person_data.loc[against_person_data['Offence Subdivision'] != 'Other crimes against the person', 'Offence Subdivision'] = \
    against_person_data.loc[against_person_data['Offence Subdivision'] != 'Other crimes against the person', 'Offence Subdivision'].str[4:]
against_person_data['Offence Subdivision'].unique()

# drop the unecessary column
against_person_data = against_person_data.drop(['Offence Subgroup'], axis =1)

# group by and sum the crime records
df_crime = against_person_data.groupby(['Year', 'Local Government Area', 'Suburb/Town Name', \
                                        'Postcode', 'Offence Subdivision'])['Incidents Recorded'].sum()
df_crime = df_crime.rename_axis(['Year', 'Local Government Area', 'Suburb/Town Name', \
                                 'Postcode', 'Offence Subdivision']).reset_index()
df_crime.head()

"""### Export Cleaned dataset"""

# save Clean data to csv
df_crime.to_csv('/content/drive/MyDrive/FIT5120 Team TA25 Project Governance Portfolio/System Architecture and Security/Data Governance/Processed Data/clean_crime_1.csv')

"""## POSTCODE"""

# read the postcode dataset
post = pd.read_csv('/content/drive/MyDrive/FIT5120 Team TA25 Project Governance Portfolio/System Architecture and Security/Data Governance/Open Dataset/au_postcodes.csv')
post.head()

"""### Check Dataset"""

# check for duplicated data in the dataset
post.duplicated().any()

# check for the missing data in the dataset
post.isnull().any()

# check for the records with the missing data
post[post['accuracy'].isnull()]

"""### Modify the dataset"""

# filter to VIC state only
VIC_post = post[post['state_code'] == 'VIC']
VIC_post.head()

# check for missing data in the filtered data
VIC_post.isnull().any()

# remove unnecessary columns 
VIC_post = VIC_post.drop(['state_name', 'state_code', 'accuracy'], axis =1)
VIC_post.head()

"""### Export dataset"""

# export clean dataset
VIC_post.to_csv('/content/drive/MyDrive/FIT5120 Team TA25 Project Governance Portfolio/System Architecture and Security/Data Governance/Processed Data/clean_postcode_1.csv')

"""## Suburb Boundary"""

# read dataset
shapefile = gpd.read_file("/content/drive/MyDrive/FIT5120 Team TA25 Project Governance Portfolio/System Architecture and Security/Data Governance/Open Dataset/vic_localities.zip")
shapefile.head()

"""### Check Data"""

# check missing data in the dataset
shapefile.isnull().any()

# check duplicated data in the dataset
shapefile.duplicated().any()

shapefile.plot()

"""### Modify the dataset"""

# drop unnecessary columns
shapefile = shapefile.drop(['LC_PLY_PID', 'LOC_PID', 'DT_CREATE', 'STATE'], axis =1)
shapefile.head()

"""### Export Dataset"""

# Export Clean Dataset
shapefile.to_csv('/content/drive/MyDrive/FIT5120 Team TA25 Project Governance Portfolio/System Architecture and Security/Data Governance/Processed Data/clean_suburb_1.csv')

"""# Join the Dataset

## Join the Suburb and Postcode Dataset
"""

# read location dataset
shapefile = pd.read_csv('/content/drive/MyDrive/FIT5120 Team TA25 Project Governance Portfolio/System Architecture and Security/Data Governance/Processed Data/clean_suburb_1.csv')
shapefile = shapefile.drop(['Unnamed: 0'], axis =1)

# convert geometry data type
shapefile['geometry'] = shapefile['geometry'].apply(wkt.loads)
shapefile = gpd.GeoDataFrame(shapefile, crs='epsg:4326')

# read postcode# export clean dataset
VIC_post = pd.read_csv('/content/drive/MyDrive/FIT5120 Team TA25 Project Governance Portfolio/System Architecture and Security/Data Governance/Processed Data/clean_postcode_1.csv')
VIC_post = VIC_post.drop(['Unnamed: 0'], axis =1)

# join suburb and postcode dataset
df_loc = shapefile[['LOC_NAME', 'geometry']].join(VIC_post.set_index('place_name'), on='LOC_NAME')
df_loc = df_loc.reset_index(drop=True)
df_loc.head()

"""## Check Data"""

# check duplicated data
df_loc.duplicated().any()

# check missing data
df_loc.isnull().any()

# check the missing data in postcode
df_loc[df_loc['postcode'].isnull()]['LOC_NAME']

# Create Dictionary of missing are postcode - from google 
dict_post_missing_area = {'Aintree' : 3336,
                          'Bend of Islands' : 3097,
                          'Bonnie Brook' : 3335,
                          'Bridgewater on Loddon': 3516,
                          'Capel Sound': 3940,
                          'Cobblebank': 3338,
                          'Creek View':3558,
                          'Deanside': 3336,
                          'Fieldstone': 3024,
                          'Fraser Rise' : 3336,
                          'Grangefields' :3335,
                          'Harkness' : 3337,
                          'Manor Lakes': 3024,
                          'McCrae' : 3938,
                          'McIntyre':3472,
                          'McKenzie Creek':3401,
                          'McKenzie Hill':3451,
                          'McKinnon' : 3204,
                          'McLoughlins Beach':3874,
                          'McMahons Creek':3799,
                          'McMillans': 3568,
                          'Strathtulloh' : 3338,
                          'Thornhill Park' : 3335,
                          'Weir Views':3338,
                          'Winter Valley' : 3358,
                          'Woodstock on Loddon': 3551}

# create list of the area with missing postcode
li_area_missing_post = df_loc[df_loc['postcode'].isnull()]['LOC_NAME'].to_list()

# impute missing postcode
for area in li_area_missing_post:
  df_loc.loc[df_loc['LOC_NAME'] == area, 'postcode'] = dict_post_missing_area[area]

# check data result
df_loc[df_loc['postcode'].isnull()]

"""## Check the latitude and longitude"""

# check the latitude longitude data
true_count = 0
false_count = 0
for i in range(len(df_loc)):
  # print(i)
  if (df_loc['geometry'][i].contains(Point(df_loc['longitude'][i], df_loc['latitude'][i]))):
    true_count += 1
  else:
    false_count += 1

print('correct data: ', true_count)
print('incorrect data: ', false_count)

# Fix the latitude longitude with the centroid location
for i in range(len(df_loc)):
  df_loc['longitude'][i] = df_loc['geometry'][i].centroid.x
  df_loc['latitude'][i] = df_loc['geometry'][i].centroid.y
df_loc.head()

"""## Create id for each area"""

df_loc.insert(loc=0, column='id', value=np.arange(1, len(df_loc) + 1))
df_loc.head()

"""## Export Joined Dataset"""

df_loc.to_csv('/content/drive/MyDrive/FIT5120 Team TA25 Project Governance Portfolio/System Architecture and Security/Data Governance/Processed Data/joined_postcode_suburb.csv')

"""# Modify The Dataset For Database"""

# read data
df_loc.to_csv('/content/drive/MyDrive/FIT5120 Team TA25 Project Governance Portfolio/System Architecture and Security/Data Governance/Processed Data/joined_postcode_suburb.csv', index=False, sep = ';')

# create temporary dataset to define the relation (id)
temp = df_loc.copy()
temp = temp.set_index(temp['postcode'].astype(int).astype(str)  + '_' + temp['LOC_NAME'].str.lower())
temp = temp.drop(['LOC_NAME', 'geometry', 'postcode', 'latitude', 'longitude'], axis = 1)
temp.head()

# convert to dictionary
dict_post_name = temp.to_dict()

"""## Crime Dataset"""

# define the key column (postcode_suburb)
df_crime['key'] = df_crime['Postcode'].astype(int).astype(str)  + '_' + df_crime['Suburb/Town Name'].str.lower()
df_crime.head()

# add the id_loc based on the postcode_suburb dictionary
df_crime['id_loc'] = df_crime.apply(lambda row : dict_post_name['id'][row['key']] if row['key'] in dict_post_name['id'].keys() else 0, axis=1) 
df_crime.head()

# DROP UNECESSARY COLUMNS
df_crime = df_crime.drop(['Postcode', 'Suburb/Town Name', 'key'], axis = 1)
df_crime.head()

# define the id for each data
df_crime['id'] = value=np.arange(1, len(df_crime) + 1)

"""### Export Dataset"""

# export dataset
df_crime.to_csv('/content/drive/MyDrive/FIT5120 Team TA25 Project Governance Portfolio/System Architecture and Security/Data Governance/Processed Data/db_crime_data.csv', index= False, sep = ';')

"""## Location Dataset

### Export Data
"""

df_loc.head()

df_loc.to_csv('/content/drive/MyDrive/FIT5120 Team TA25 Project Governance Portfolio/System Architecture and Security/Data Governance/Processed Data/db_postcode_data.csv', index=False, sep = ';')

"""# Data Exploration"""

# read location data
df_loc = pd.read_csv('/content/drive/MyDrive/IE/CleanData/db_postcode_data.csv', sep=';')

# convert data type
df_loc['geometry'] = df_loc['geometry'].apply(wkt.loads)
df_loc = gpd.GeoDataFrame(df_loc, crs='epsg:4326')
df_loc['postcode'] = df_loc['postcode'].astype(int)

# read crime data
df_crime = pd.read_csv('/content/drive/MyDrive/IE/CleanData/db_crime_data.csv', sep=';')

# drop unnamed: 0 column
df_crime = df_crime.drop(['id'], axis =1)
df_crime.head()

# join data for exploration 
df = df_loc.join(df_crime.set_index('id_loc'), on='id')
df = df.drop(['id'], axis =1)

# fill the null data because of joining process
# df[df['Year'].isnull()]
df['Year'].fillna(0, inplace=True)
df['Offence Subdivision'].fillna(0, inplace=True)
df['Incidents Recorded'].fillna(0, inplace=True)
df.head()

"""## Average crime in 10 years"""

# copy the dataset
df_avg_crime = df_crime.copy()

# group by location
df_avg_crime = df_avg_crime.groupby(['id_loc'])['Incidents Recorded'].sum()
df_avg_crime = df_avg_crime.rename_axis(['id_loc']).reset_index()

# take the average of 10 years (2013 - 2022)
df_avg_crime['avg_crime'] = df_avg_crime['Incidents Recorded']/10

# sort by incident records
# take the top 10
df_avg_crime.sort_values(by = 'avg_crime', ascending = False).head(10)

# take the top 10 location id of highest crime records 
li_top_10_suburb_crime = df_avg_crime.sort_values(by = 'avg_crime', ascending = False).head(10)['id_loc'].to_list()

"""### Show top 10 highest crime suburb"""

# crime in 2022
df_top_10 = df_crime.loc[df_crime['id_loc'].isin(li_top_10_suburb_crime)]

# take the top 10 area
df_top_10_map = df_loc.join(df_top_10.set_index('id_loc'), on='id')
df_top_10_map = df_top_10_map.drop(['id'], axis =1)

# fill the nulls
df_top_10_map[df_top_10_map['Year'].isnull()]
df_top_10_map['Year'].fillna(0, inplace=True)
df_top_10_map['Offence Subdivision'].fillna(0, inplace=True)
df_top_10_map['Incidents Recorded'].fillna(0, inplace=True)
df_top_10_map

"""### Top 10 area with highest crime records"""

# set figure size
figsize = (20, 11)

# visualise the area with top 10 highest crime records
df_top_10_map = df_top_10_map[df_top_10_map['Incidents Recorded'] > 1]
df_top_10_map.plot('Incidents Recorded', legend=True, figsize=figsize);

"""### Crime in Map visualisation"""

# filter crime in 2022
df_2022 = df_loc.join(df_crime[df_crime['Year'] == 2022].set_index('id_loc'), on='id')
df_2022 = df_2022.drop(['id'], axis =1)

# fill the nulls
df_2022[df_2022['Year'].isnull()]
df_2022['Year'].fillna(0, inplace=True)
df_2022['Offence Subdivision'].fillna(0, inplace=True)
df_2022['Incidents Recorded'].fillna(0, inplace=True)

# visualise in map 
df_2022.plot('Incidents Recorded', legend=True, figsize=figsize);

"""### crime trend in top 10 suburb"""

# drop geometry column from top 10 area
df_top_10_map_export = df_top_10_map.drop(['geometry', 'latitude', 'longitude'], axis =1)

# change type of Year
df_top_10_map_export['Year'] = df_top_10_map_export['Year'].astype(int)
df_top_10_map_export.head()

"""#### visualisation of all crime records (sum of all type)"""

# group the data bsed on year and location
df_type_year_top_10 = df_top_10_map_export.groupby(['Year', 'LOC_NAME'])['Incidents Recorded'].sum()
df_type_year_top_10 = df_type_year_top_10.rename_axis(['Year', 'LOC_NAME']).reset_index()

# visualise the trend of crime in each area
for t in df_type_year_top_10['LOC_NAME'].unique():
  plt.plot(df_type_year_top_10['Year'].unique(), df_type_year_top_10[df_type_year_top_10['LOC_NAME']== t]['Incidents Recorded'].to_list(), 'o--',  alpha=0.3, label=t)

plt.grid(axis='x', color='0.95')
plt.legend(title='Suburb', loc='center left', bbox_to_anchor=(1, 0.5))
plt.title('Crime record in top 10 suburb')
plt.show()

"""#### visualisation of each crime type"""

# set figure setting
fig = plt.figure(figsize=(20,8)) 
axs = fig.subplots(4,3)
row = 0 
col = 0

# plot the crime type trend on each area
for s in df_top_10_map_export['LOC_NAME'].unique():
  for t in df_top_10_map_export['Offence Subdivision'].unique():
    if (col == 3):
      row += 1
      col = 0
    axs[row, col].plot(df_top_10_map_export['Year'].unique(), df_top_10_map_export[(df_top_10_map_export['LOC_NAME']== s) & (df_top_10_map_export['Offence Subdivision']== t)]['Incidents Recorded'].to_list(), 'o--',  alpha=0.3, label=t)
    axs[row, col].set_title(s)
    fig.legend(title='type', loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True)
  col+=1

"""### Crime type trend"""

# group by the type subdivision
df_type_year = df_crime.groupby(['Year', 'Offence Subdivision'])['Incidents Recorded'].sum()
df_type_year = df_type_year.rename_axis(['Year', 'Offence Subdivision']).reset_index()

# figure setting
fig, ax = plt.subplots()

# plot in barplot
for t in df_type_year['Offence Subdivision'].unique():
  ax.bar(df_type_year['Year'].unique(), df_type_year[df_type_year['Offence Subdivision']== t]['Incidents Recorded'].to_list(), label=t)

ax.set_ylabel('Year')
ax.set_title('Crime Records')
ax.legend()
plt.show()

# plot each type in line graph
for t in df_type_year['Offence Subdivision'].unique():
  plt.plot(df_type_year['Year'].unique(), df_type_year[df_type_year['Offence Subdivision']== t]['Incidents Recorded'].to_list(), 'o--',  alpha=0.3, label=t)
plt.grid(axis='x', color='0.95')
plt.legend(title='Parameter where:')
plt.title('plt.step(where=...)')
plt.show()

# figure setting
fig1, ax1 = plt.subplots()

# visualisation in pie chart
ax1.pie(df_type_year[df_type_year['Year'] == 2022]['Incidents Recorded'], labels=df_type_year[df_type_year['Year'] == 2022]['Offence Subdivision'], autopct='%1.1f%%',
        shadow=True, startangle=90)
ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

plt.title('Crime Type')
plt.show()

"""#### Conclusion
> Compared to the other defence types, the 'Stalking, harassment and threatening behaviour' records constantly increasing since 2013. Be careful~

> More than half of the crimes that happen is about assault and offences

### Crime records trends in Victoria
"""

# figure setting
fig, ax = plt.subplots()

# barplot og stalking, harassment and threatening behaviour crime in Victoria 
ax.bar(df_type_year['Year'].unique(), df_type_year[df_type_year['Offence Subdivision']== 'Stalking, harassment and threatening behaviour']['Incidents Recorded'].to_list(), label='Stalking, harassment and threatening behaviour')
ax.set_ylabel('Year')
ax.set_title('Crime Records')
ax.legend()
plt.show()

# data detail of 'Stalking, harassment and threatening behaviour' crime
df_type_year[df_type_year['Offence Subdivision']== 'Stalking, harassment and threatening behaviour']['Incidents Recorded'].to_list()

"""### Criminal Incident rate

Criminal incident rate = (Criminal incident count/ERP count) *100,000
"""

# sum crime records in each year
df_year = df_crime.groupby(['Year'])['Incidents Recorded'].sum()
df_year.head()

# make the list of crime rate that happen in each year
li_rate_year = []

# calculate the crime rate in each year
for each in df_year:
  print(each/66743290*100000)
  li_rate_year.append(each/66743290*100000)

"""### Crime trend in past 10 years in Victoria"""

# group the records by year and type
data_1 = df_type_year.groupby(['Year'])['Incidents Recorded'].sum()
data_1 = data_1.rename_axis(['Year']).reset_index()

# plot the data in line plot
plt.plot(data_1['Year'].unique(), data_1['Incidents Recorded'].to_list(), 'o--',  alpha=0.3, label=t)
plt.grid(axis='x', color='0.95')
plt.title('Crime in Victoria in 10 years')
plt.show()

print('data detail: ')
print(data_1)

"""### Crime that happen in Victoria each hour, minute, second"""

a = against_person_data[against_person_data['Offence Division'] == 'Crimes against the person'].groupby(['Year'])['Incidents Recorded'].sum()
a = a.rename_axis(['Year']).reset_index()

# Crime records increase 
print('Crime records increment in 10 years: ')
print(a['Incidents Recorded'][9] - a['Incidents Recorded'][0])
print()

# rate of increase in 10 years
print('crime rate increase in 10 years: ')
print((a['Incidents Recorded'][9] - a['Incidents Recorded'][0]) / a['Incidents Recorded'][9])
print()

# incident that happen in each second 
print('incident that happen in each second: ')
print(a['Incidents Recorded'][9] / 31536000)
print()

# incident that happen in each minute 
print('incident that happen in each minute: ')
print(a['Incidents Recorded'][9] / 525600)
print()

print('incident that happen in each hour')
print(a['Incidents Recorded'][9] / 8760)

"""#### Conclusion


> The crime against other person in Victoria has increased by 26% in the past 10 years

> There are at least 1 people harrased in every minute.

### Crime by age group
"""

# read dataset of crime with age details
data_age = pd.read_excel('/content/drive/MyDrive/FIT5120 Team TA25 Project Governance Portfolio/System Architecture and Security/Data Governance/Open Dataset/Data_Tables_LGA_Victim_Reports_Year_Ending_March_2022.xlsx', sheet_name='Table 03')
data_age.head()

# group by age range and year
data_age_year = data_age.groupby(['Year', 'Age Group'])['Victim Reports'].sum()
data_age_year = data_age_year.rename_axis(['Year', 'Age Group']).reset_index()

# visualise in line plot
for t in data_age_year['Age Group'].unique():
  plt.plot(data_age_year['Year'].unique(), data_age_year[data_age_year['Age Group']== t]['Victim Reports'].to_list(), 'o--',  alpha=0.3, label=t)
plt.grid(axis='x', color='0.95')
plt.legend(title='crime type', loc='center left', bbox_to_anchor=(1, 0.5))
plt.title('Crime in 10 years')
plt.show()

# average of crime reported by each age group 
data_age_avg = data_age.groupby(['Age Group'])['Victim Reports'].sum()
data_age_avg = data_age_avg.rename_axis(['Age Group']).reset_index()
data_age_avg['Victim Reports'] = data_age_avg['Victim Reports'] / 10
data_age_avg

"""### Crime by Gender"""

# read dataset of crime with age details
data_sex = pd.read_excel('/content/drive/MyDrive/IE/WomanData/Data_Tables_LGA_Victim_Reports_Year_Ending_March_2022.xlsx', sheet_name='Table 04')
data_sex.head()

# group by gender and year
data_sex_year = data_sex.groupby(['Year', 'Sex'])['Victim Reports'].sum()
data_sex_year = data_sex_year.rename_axis(['Year', 'Sex']).reset_index()

# plot in line graph
for t in data_sex_year['Sex'].unique():
  plt.plot(data_sex_year['Year'].unique(), data_sex_year[data_sex_year['Sex']== t]['Victim Reports'].to_list(), 'o--',  alpha=0.3, label=t)
plt.grid(axis='x', color='0.95')
plt.legend(title='crime type', loc='center left', bbox_to_anchor=(1, 0.5))
plt.title('Crime in 10 years')
plt.show()